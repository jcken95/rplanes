---
title: "`rplanes` Interpretations and Limitations"
output: 
  rmarkdown::html_vignette:
    toc: TRUE
    number_sections: true
vignette: >
  %\VignetteIndexEntry{PLANES Interpretations and Limitations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo=TRUE,
  comment = "#>",
  warning=FALSE,
  message=FALSE,
  fig.width = 7,
  fig.height = 5
)

library(magrittr)
library(dplyr)
```



# Overview

The `rplanes` package includes vignettes detailing [basic usage](basic-usage.html) and descriptions of the [individual components](planes-components.html) that the package uses for plausibility analysis. This vignette focuses on interpreting `rplanes` plausibility outputs. The content includes a primer on the weighting scheme for plausibility components, a discussion of limitations that may arise from relying on seed data, and considerations for what to do when a flag is raised.

# Weighting scheme within `plane_score()`

The `plane_score()` function allows users to evaluate multiple plausibility components simultaneously (i.e., without having to call individual scoring functions for each given component). This wrapper returns an overall score to summarize all components evaluated. The `plane_score()` function includes an optional "weights" argument that allows user-specified weighting of components in the overall score. By default (`weights = NULL`), each component is given equal weighting. To optionally weight certain components higher or lower, the user must specify a named vector with each value representing the weight given to each component. The length of the vector must equal the number of components used in the `components` argument of `plane_score()`. For more technical details about `plane_score()` or how to apply the weighting scheme, users should refer to the function documentation (`?plane_score`) or the [basic usage](basic-usage.html) vignette. 

## Motivations for weighting

The weighting scheme is incorporated because scores may be context-dependent. In other words, users may have varying concerns about specific components being evaluated given the timing, historical data patterns, and specific goals of their plausibility assessment. Below we have included several examples highlighting use-cases for applying the weighting scheme to `plane_score()`:

- If users are retrospectively analyzing forecast signals after the ground-truth, observed data for that horizon has been reported, they may be aware that a large jump in reported cases actually occurred. In this scenario, they might be less interested in the *difference* component, which evaluates the forecast and raises a flag if there is a point-to-point difference greater than any difference found in the observed seed. However, the users might still be concerned about any exorbitant jumps in forecasted cases. Rather than eliminating the difference component altogether, the users can reduce its weight within `plane_score()`.
- During an expected large uptick in cases, users might increase the weight of the *trend* component to more heavily penalize unexpected dips in cases. This ensures that significant trends are given the appropriate emphasis in the analysis.
- At the beginning of a season, when there tend to be many zeros, users might decide to reduce the relative weight of the *zero* and *repeat* components. This adjustment may help account for the seasonality and expected patterns in the data.
- When working with relatively short time-series seed data that only covers several months, users may encounter many shape flags due to the limited number of shapes found in the seed data. In such cases, users can either decrease the sensitivity of the *shape* component or reduce its relative weight to avoid excessive flagging.
- If evaluating forecasts operationally, users may be more interested in ensuring that prediction intervals are appropriately calibrated (e.g., not too narrow). In this case, the *cover* and *taper* components might merit higher weighting. 

# Limitations that arise from seed data

As described in the [basic usage](basic-usage.html) vignette, the `rplanes` plausibility analysis procedure requires establishing a "seed" object based on an observed signal. The seed data serves as the basis for the background characteristics used to assess plausibility. As such, plausibility results will depend upon the reliability and length of the time series used to establish the seed data. Here we discuss how both of these issues could manifest in the seed and their potential impact on plausibility analysis with `rplanes`.

## Reliability of seed data

The plausibility analysis in `rplanes` assumes that users have access to observed data to establish baseline characteristics of the time series. Presumably, this observed signal is trustworthy and is a faithful representation of what we should expect from the signal to be evaluated. However, in practice data issues such as lagged reporting, backfill, and any other systematic bias in ascertainment may lead to unexpected behaviors in `rplanes` (i.e., too many or too few flags). If there are known. If there are known issues within the seed data, users should carefully consider all [individual components](planes-components.html) used in the plausibility scoring and determine how these issues might impact the results of their plausibility analysis. For example, consider observed data that may lack consistent reporting in certain locations, particularly early on in the time series. In this case, the seed data might contain many consecutive zeros across a long time-span followed by more reliable reporting at these locations. In this case, `rplanes` would rarely (if ever) raise flags for the *repeat* and *zero* components. In such cases, truncating the observed data to begin when reporting becomes more reliable prior to creating the seed may be appropriate. 

## Length of seed data

Besides the reliability, the length (i.e., number of observations) of the observed signal used to create the seed can influence the `rplanes` plausibility scoring. In general, more data provides higher resolution for the characteristics that could manifest in evaluated data. However, users should be aware of computational costs and a potential reduction in sensitivity in some components as the length of the observed seed data increases. Here we provide a several considerations and examples of balancing length of observed used to create a seed. 

In scenarios when the seed has been created with a relatively small number of observations, users may notice that some components have higher sensitivity resulting in more flags raised. Some of the individual components have a required seed length to signal length ratio that must be met for the function to run (e.g., *shape* requires that the seed is at least four times the length of the forecast being evaluated). However, even with a built in minimum length, there are cases where the seed data may be too short to produce reasonable results. For example, consider evaluating a forecast of four horizons ahead. In this case, the seed must contain at least 16 weekly observations for the given location in this case. However, 16 weeks is roughly four months of data, which (depending on seasonality and timing of the observations) may not adequately capture all of the shapes that could plausibly manifest four weeks into the future. Below is a list of some potential complications caused by a seed object that is too short. For all of these possible issues, we recommend that users should manually examine flagged locations for plausibility and consider reducing their relative weights within the `plane_score()` function.


```{r, eval=TRUE, echo=FALSE}
tibble(
  `Component` = c("Difference", "Repeat", "Shape", "Zero"),
  `Description` = c("Point-to-point difference", "Values repeat more than expected", "Shape of signal trajectory has not been observed in seed data", "Zeros found in signal when not in seed"),
  `Issue` = c("Without enough point-to-point differences evaluated in the seed data, a true and reasonable jump in the signal may be flagged as implausible.", "If there are, by chance, no repeats in the seed, a single repeat will not be tolerated and will be flagged (also true for very few repeats). Decreasing the prepend length and/or increasing the repeat tolerance should help mitigate this.", "A short seed object is comprised of fewer signal trajectory shapes that can be compared to the signal, so a reasonable trajectory might be erroneously flagged.", "If there are no zeros in the seed, a single zero in the signal will not be tolerated and will be flagged.")
) %>%
  knitr::kable()
```
  
  
  
### Seed Data is too Long:
If the seed data is too long, youâ€™ll see the opposite behaviors with the same components (`plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()`). They will become less sensitive, and you may see fewer flags. Specifically, for `plane_repeat()`, increasing the `prepend` length and decreasing the repeat tolerance should increase the sensitivity of this component. Unlike situations when the seed data is too short, changing the relative weights of these components will not have as much of an impact on overall scores. Down-weighting the components when using a short seed object causes potentially erroneous flags to have less of an effect on the overall score, however increasing the weights will not cause more flags to be raised (i.e., changing the weights does not change the sensitivity of components). Further, manual inspection of "negative flags" is much more challenging and time-consuming. Instead, we suggest truncating the seed data if `plane_score()` is producing unreliable results. Truncating the seed data has the added benefit of reducing computational cost, but users should perform sensitivity analyses to arrive at the ideal length for their seed data.
  
  
#### Real-World COVID Example
A very long time-series seed object, covering multiple seasons of epidemiological signals for the same target, reduces the sensitivity of some components. For example, if `plane_shape()` is being used, a longer seed time-series will contain more unique shapes, resulting in fewer novel shapes in forecasts and fewer flags being raised. Having observed a similar epidemiological signal before is not (alone) enough justification to infer that this shape is not unusual and should not be flagged. COVID forecasts in 2024 with trajectories mimicking signals from 2020 should be flagged as implausible. If a user's reference seed data included pandemic case counts however, it would not be flagged.



# What to Do When a Flag is Raised:
When a flag is raised, the following steps are recommended:

1. Manually inspect the signals by plotting the observed seed data along with the signal being evaluated.
2. If many flags are raised and the signal appears implausible to subject matter experts, users can likely accept the plausibility score and censor that forecast from your downstream analyses. The score could also be used as a downstream weight for this forecast (in an ensemble model for example).
3. If flags are raised but the signal does not appear implausible, inspect the individual flagged components. Adjusting the arguments for `plane_repeat()` and `plane_trend()` can increase or decrease their sensitivity. Short seed data can cause certain components to be overly-sensitive (`plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()`), and users can either weight these components less within `plane_score()` or remove them altogether.

Retrospective analysis of scoring and sensitivity before interpreting `plane_score()` output can help determine normal flag frequencies for specific signals, such as Lyme disease versus influenza or influenza in Florida versus New York.


# Key Takeaways:
There are myriad reasons that a user might want to change the relative weights of individual components or leave components out of the `plane_score()` function altogether. Manual inspection of all raised flags is crucial, because flags can be raised erroneously for all of the reasons discussed in this vignette. Further, we recommend selecting (or simulating) a few signals that you would expect to trigger flags in `plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()` for calibration purposes. These components are most affected by the length and quality of the seed data. Additionally, calibration of the significance level argument in `plane_trend()` is suggested to accurately capture unexpected inflection points in forecast signals.



