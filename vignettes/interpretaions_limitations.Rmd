---
title: "`rplanes` Interpretations and Limitations"
output: 
  rmarkdown::html_vignette:
    toc: TRUE
    number_sections: true
vignette: >
  %\VignetteIndexEntry{PLANES Interpretations and Limitations}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo=TRUE,
  comment = "#>",
  warning=FALSE,
  message=FALSE,
  fig.width = 7,
  fig.height = 5
)

library(magrittr)
library(dplyr)
```



# Overview

The `rplanes` package includes vignettes detailing [basic usage](basic-usage.html) and descriptions of the [individual components](planes-components.html) that the package uses for plausibility analysis. This vignette focuses on interpreting `rplanes` plausibility outputs. The content includes a primer on the weighting scheme for plausibility components, a discussion of limitations that may arise from relying on seed data, and considerations for what to do when a flag is raised.

# Weighting scheme within `plane_score()`

The `plane_score()` function allows users to evaluate multiple plausibility components simultaneously (i.e., without having to call individual scoring functions for each given component). This wrapper returns an overall score to summarize all components evaluated. The `plane_score()` function includes an optional "weights" argument that allows user-specified weighting of components in the overall score. By default (`weights = NULL`), each component is given equal weighting. To optionally weight certain components higher or lower, the user must specify a named vector with each value representing the weight given to each component. The length of the vector must equal the number of components used in the `components` argument of `plane_score()`. For more technical details about `plane_score()` or how to apply the weighting scheme, users should refer to the function documentation (`?plane_score`) or the [basic usage](basic-usage.html) vignette. 

## Motivations for weighting

The weighting scheme is incorporated because scores may be context-dependent. In other words, users may have varying concerns about specific components being evaluated given the timing, historical data patterns, and specific goals of their plausibility assessment. Below we have included several examples highlighting use-cases for applying the weighting scheme to `plane_score()`:

- If users are retrospectively analyzing forecast signals after the ground-truth, observed data for that horizon has been reported, they may be aware that a large jump in reported cases actually occurred. In this scenario, they might be less interested in the *difference* component, which evaluates the forecast and raises a flag if there is a point-to-point difference greater than any difference found in the observed seed. However, the users might still be concerned about any exorbitant jumps in forecasted cases. Rather than eliminating the difference component altogether, the users can reduce its weight within `plane_score()`.
- During an expected large uptick in cases, users might increase the weight of the *trend* component to more heavily penalize unexpected dips in cases. This ensures that significant trends are given the appropriate emphasis in the analysis.
- At the beginning of a season, when there tend to be many zeros, users might decide to reduce the relative weight of the *zero* and *repeat* components. This adjustment may help account for the seasonality and expected patterns in the data.
- When working with relatively short time-series seed data that only covers several months, users may encounter many shape flags due to the limited number of shapes found in the seed data. In such cases, users can either decrease the sensitivity of the *shape* component or reduce its relative weight to avoid excessive flagging.
- If evaluating forecasts operationally, users may be more interested in ensuring that prediction intervals are appropriately calibrated (e.g., not too narrow). In this case, the *cover* and *taper* components might merit higher weighting. 

# Limitations that Arise from Seed Data:

Several considerations must be made when thinking about the observed data that is used as the seed object (see [Basic Usage vignette](basic-usage.html) for instructions related to creating a seed object). The seed data serves as the reference truth for our components, so results will depend upon the reliability and length of the seed data.



## Reliability of Seed Data:
Unreliable seed data, such as data with reporting issues, backfill, manual entry mistakes, biases, or any other problems that make it less reliable than ideal, can cause erroneous behaviors in `rplanes` (i.e., too many or too few flags). If there are known issues within the seed data, users should carefully consider all [individual components](planes-components.html) used in `plane_score()` and assess whether these issues will bias the results of their plausibility analysis. 

One example of an issue with unreliable seed data is lack of reporting in certain locations. Some training seed data may contain many consecutive zeros across a long time-span followed by more reliable reporting. In this case, `rplanes` would rarely (if ever) raise flags for too many repeats, `plane_repeat()`, or zeros, `plane_zero()`. In such cases, truncating the seed data to begin when reporting becomes more reliable should improve performance.



## Length of Seed Data:
In general, more training data provides more reliable results, but users should consider computational costs and a potential reduction in sensitivity in some components as the length of the observed seed data increases.



### Seed Data is too Short: 
Short seed data can cause higher sensitivity in some components and result in more flags. Most of the individual components have a required seed length to signal length ratio that must be met for the function to run (e.g., `plane_shape()` requires that the seed is at least 4 times the length of the forecast being evaluated). However, even with a built in minimum length, there are cases where the seed data may be too short to produce reasonable results. When the seed data is shorter, `plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()` become more sensitive and will therefore produce more flags.

Below is a list of possible complications caused by a seed object that is too short. For all of these potential issues, you should manually examine flagged locations for plausibility and consider reducing their relative weights within the `plane_score()` function.


```{r, eval=TRUE, echo=FALSE}
tibble(
  `Component` = c("Difference", "Repeat", "Shape", "Zero"),
  `Function Description` = c("Point-to-point difference", "Values repeat more than expected", "Shape of signal trajectory has not been observed in seed data", "Zeros found in signal when not in seed"),
  `Reason for Issue` = c("Without enough point-to-point differences evaluated in the seed data, a true and reasonable jump in the signal may be flagged as implausible.", "If there are, by chance, no repeats in the seed, a single repeat will not be tolerated and will be flagged (also true for very few repeats). Decreasing the prepend length and/or increasing the repeat tolerance should help mitigate this.", "A short seed object is comprised of fewer signal trajectory shapes that can be compared to the signal, so a reasonable trajectory might be erroneously flagged.", "If there are no zeros in the seed, a single zero in the signal will not be tolerated and will be flagged")
) %>%
  knitr::kable()
```
  
  
  
### Seed Data is too Long:
If the seed data is too long, youâ€™ll see the opposite behaviors with the same components (`plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()`). They will become less sensitive, and you may see fewer flags. Specifically, for `plane_repeat()`, increasing the `prepend` length and decreasing the repeat tolerance should increase the sensitivity of this component. Unlike situations when the seed data is too short, changing the relative weights of these components will not have as much of an impact on overall scores. Down-weighting the components when using a short seed object causes potentially erroneous flags to have less of an effect on the overall score, however increasing the weights will not cause more flags to be raised (i.e., changing the weights does not change the sensitivity of components). Further, manual inspection of "negative flags" is much more challenging and time-consuming. Instead, we suggest truncating the seed data if `plane_score()` is producing unreliable results. Truncating the seed data has the added benefit of reducing computational cost, but users should perform sensitivity analyses to arrive at the ideal length for their seed data.
  
  
#### Real-World COVID Example
A very long time-series seed object, covering multiple seasons of epidemiological signals for the same target, reduces the sensitivity of some components. For example, if `plane_shape()` is being used, a longer seed time-series will contain more unique shapes, resulting in fewer novel shapes in forecasts and fewer flags being raised. Having observed a similar epidemiological signal before is not (alone) enough justification to infer that this shape is not unusual and should not be flagged. COVID forecasts in 2024 with trajectories mimicking signals from 2020 should be flagged as implausible. If a user's reference seed data included pandemic case counts however, it would not be flagged.



# What to Do When a Flag is Raised:
When a flag is raised, the following steps are recommended:

1. Manually inspect the signals by plotting the observed seed data along with the signal being evaluated.
2. If many flags are raised and the signal appears implausible to subject matter experts, users can likely accept the plausibility score and censor that forecast from your downstream analyses. The score could also be used as a downstream weight for this forecast (in an ensemble model for example).
3. If flags are raised but the signal does not appear implausible, inspect the individual flagged components. Adjusting the arguments for `plane_repeat()` and `plane_trend()` can increase or decrease their sensitivity. Short seed data can cause certain components to be overly-sensitive (`plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()`), and users can either weight these components less within `plane_score()` or remove them altogether.

Retrospective analysis of scoring and sensitivity before interpreting `plane_score()` output can help determine normal flag frequencies for specific signals, such as Lyme disease versus influenza or influenza in Florida versus New York.


# Key Takeaways:
There are myriad reasons that a user might want to change the relative weights of individual components or leave components out of the `plane_score()` function altogether. Manual inspection of all raised flags is crucial, because flags can be raised erroneously for all of the reasons discussed in this vignette. Further, we recommend selecting (or simulating) a few signals that you would expect to trigger flags in `plane_diff()`, `plane_repeat()`, `plane_shape()`, and `plane_zero()` for calibration purposes. These components are most affected by the length and quality of the seed data. Additionally, calibration of the significance level argument in `plane_trend()` is suggested to accurately capture unexpected inflection points in forecast signals.



